# -*- coding: utf-8 -*-
"""
Created on Mon Jul 19 16:55:09 2021

@author: ndbke
"""


import numpy as np
import matplotlib.pyplot as plt
import scipy.optimize as optimize
import xarray as xr
import matplotlib.colors as colors

# ignore runtimewarnings generated by Newton's Method searching invalid areas
import warnings
warnings.filterwarnings("ignore")#, message="invalid value encountered in power")




### MEAN constraint for Lagrange multiplier
def mean_con(beta,X,Xavg):
    
    num = 0
    denom = 0
    for i in range(len(X)):
        num += X[i] * np.exp(-beta*X[i])
        denom += np.exp(-beta*X[i])
        
    return num / denom - Xavg



### VARIANCE constraint for Lagrange multiplier
def var_con(delta,X,Xavg,Xvar):
    
    num = 0
    denom = 0
    for i in range(0,len(X)):
        et = np.exp( - delta * (X[i]-Xavg)**2 )
        num += et * (X[i] - Xavg)**2
        denom += et
        
    return num / denom - Xvar
    
        
    
    
    
    


### entropy maximization w. 1-vbl MEAN
def max_1m(X,Xavg):
    
    # adapt range of guesses to the inputted value
    guesses = np.linspace(0.001,50,200)
    # explore for roots across guess space
    roots = optimize.newton(mean_con, x0=guesses, args=(X,Xavg),maxiter=1000,full_output=True)
    roots = clean_roots(roots)
    
    if roots.size == 0:
        print('no roots')
        return
    
    # get beta and alpha from root
    beta = roots[0]
    print(beta)
    alpha = np.log( np.sum( np.exp(-beta * X) ) )
    
    # get 
    p_arr = np.empty((len(X),1))
    for i in range(0,len(X)):
        p_arr[i] = np.exp( -alpha - beta * X[i] )
    p_arr /= np.sum(p_arr)
    
    return roots, beta, p_arr




### entropy maximization w. 2-vbl MEANS
def max_2m(X,Xavg,Y,Yavg):
    
    # adapt range of guesses to the inputted value
    guesses = np.column_stack((np.linspace(0.001,50,200),np.linspace(0.001,50,200))).T
    
    # explore for roots across guess space
    roots_x = optimize.newton(mean_con, x0=guesses, args=(X,Xavg),maxiter=1000,full_output=True)
    roots_x = clean_roots(roots_x)
    
    roots_y = optimize.newton(mean_con, x0=guesses, args=(Y,Yavg),maxiter=1000,full_output=True)
    roots_y = clean_roots(roots_y)
    
    # define Lagrange multipliers
    beta = roots_x[0]
    gamma = roots_y[0]
    alpha = np.log( np.sum(np.exp( - beta * X)) * np.sum(np.exp(- gamma * Y)) )

    # get max-con-ent distribution
    p_arr = np.empty((len(Y),len(X)))
    for i in range(0,len(Y)):
        for j in range(0,len(X)):
            p_arr[i,j] = np.exp( -alpha - beta * X[j] - gamma * Y[i] )
    p_arr /= np.sum(p_arr)

    return [roots_x,roots_y], [alpha,beta,gamma], p_arr




### entropy maximization w. 1-vbl VARIANCE
def max_1v(X,Xavg,Xvar):
    
    # adapt range of guesses to the inputted value
    guesses = np.linspace(0.001,50,200)
    
    # find roots -> delta
    roots_d = optimize.newton(var_con, x0=guesses, args=(X,Xavg,Xvar),maxiter=1000,full_output=True)
    delta = clean_roots(roots_d)[0]
    
    # get alpha from beta, delta via distribution function Z
    alpha = np.log( np.sum( np.exp(- delta * (X-Xavg)**2 )))

    # max entropy distribution
    p_arr = np.empty((len(X),1))
    for i in range(0,len(X)):
        p_arr[i] = np.exp( -alpha - delta * (X[i] - Xavg)**2)
    p_arr /= np.sum(p_arr)
    
    return p_arr
    



### entropy maximization w. 2-vbl VARIANCE
def max_2v(X,Xavg,Xvar,Y,Yavg,Yvar):
    
    # adapt range of guesses to the inputted value
    guesses = np.linspace(0.001,50,200)
    
    # find roots -> delta
    roots_d = optimize.newton(var_con, x0=guesses, args=(X,Xavg,Xvar),maxiter=1000,full_output=True)
    delta = clean_roots(roots_d)[0]
    roots_e = optimize.newton(var_con, x0=guesses, args=(Y,Yavg,Yvar),maxiter=1000,full_output=True)
    epsilon = clean_roots(roots_e)[0]
    
    # get alpha from beta, delta via distribution function Z
    alpha = np.log( np.sum( np.exp(- delta * (X-Xavg)**2 )) * np.sum( np.exp( - epsilon * (Y-Yavg)**2 )))

    p_arr = np.empty((len(Y),len(X)))
    for i in range(0,len(Y)):
        for j in range(0,len(X)):
            p_arr[i,j] = np.exp( -alpha - delta * (X[j]-Xavg)**2 - epsilon * (Y[i]-Yavg)**2 )
    p_arr /= np.sum(p_arr)
    
    return p_arr
    


### find actual roots output by Newton's Method
def clean_roots(roots_obj):
    
    # only roots for which alg converged
    roots = roots_obj.root[roots_obj.converged] 
    
    # remove NaNs
    roots = roots[~np.isnan(roots)] 

    # round off to remove tiny differences between what are actually same roots         
    roots = np.round(roots,5)                   
    
    # get unique values
    roots = np.unique(roots) 
                 
    return roots
    
    
    
### get averages from volumetric T-S plot
def tsv_dist(tsv):
    
    p_T = tsv.sum('s')
    p_S = tsv.sum('t')
    p_T /= np.sum(p_T)
    p_S /= np.sum(p_S)
    
    T_avg = np.average(tsv.t.values,weights=p_T)
    S_avg = np.average(tsv.s.values,weights=p_S)

    return T_avg, S_avg



#%% inputs

tsv_arc = xr.open_dataset('../NetCDFs/tsv_arc.nc', decode_times=False, autoclose=True)
T = tsv_arc.t.values
S = tsv_arc.s.values
T_avg, S_avg = tsv_dist(tsv_arc.volume)


X = S
Xavg = S_avg
Y = T
Yavg = T_avg



## test values
# X = np.linspace(-2,12,10)
# Xavg = 3
# Y = np.linspace(32,35,10)
# Yavg = 34



#%% UNIVARIATE - mean


# 1D maximization
xroot, xbeta, xpam = max_1m(X,Xavg)
yroot, ybeta, ypam = max_1m(Y,Yavg)
fx = mean_con(xroot,X,Xavg)
fy = mean_con(yroot,Y,Yavg)

# plot 1D solutions
plt.figure()
plt.plot(X,xpam)
plt.figure()
plt.plot(Y,ypam)




#%% BIVARIATE - mean

# 2D maximization
roots, mults, p_arr = max_2m(X,Xavg,Y,Yavg)

# plot 2D solution in logscale colors
plt.figure()
plt.pcolormesh(X,Y,p_arr,shading="nearest",norm=colors.LogNorm())
plt.colorbar()





#%% UNIVARIATE - mean & variance

Xvar = 0.25
Yvar = 1

p_arr = max_1v(Y,Yavg,Yvar)

plt.figure()
plt.plot(Y,p_arr)




#%% BIVARIATE - mean & variance

p_arr = max_2v(X,Xavg,Xvar,Y,Yavg,Yvar)
plt.figure()
plt.pcolormesh(X,Y,p_arr,norm=colors.LogNorm())
